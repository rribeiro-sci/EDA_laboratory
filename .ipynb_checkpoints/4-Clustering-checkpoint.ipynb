{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202eb0b7-c248-4328-b0fa-6057e26dff12",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:50px; font-family:Helvetica ; font-weight : normal; color : #fe346e; text-align: center;\"> Exploratory Data Analysis</h1>\n",
    "<h2 style = \"font-size:40px; font-family:Helvetica ; font-weight : normal; text-align: center;\"> Clustering </h2>\n",
    "<br><br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612126/EDA/dw1xwbh1g2c85izletjs.png' width=\"200\"  style=\"float:center\" align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style='padding:15px'>\n",
    "<a href=\"https://colab.research.google.com/github/rribeiro-sci/EDA_laboratory/blob/main/4-Clustering.ipynb\" target=\"_blank\">\n",
    "<img alt=\"Colab\" src=\"https://res.cloudinary.com/djz27k5hg/image/upload/v1637335713/badges/colab-badge_hh0uyl.svg\" height=\"25\" style=\"margin:20px\">\n",
    "</a>\n",
    "\n",
    "</div> \n",
    "\n",
    "\n",
    "**Clustering** is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. \n",
    "\n",
    "Clustering is considered an unsupervised learning method since we donâ€™t have the ground truth to compare the output of the clustering algorithm to the true labels to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174737c-4858-4277-a490-5859be8a3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# importing iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# loading iris dataset into a dataframe\n",
    "species = np.array([iris.target_names[val] for val in iris.target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da78f5",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "K-means is an unsupervised clustering algorithm designed to partition unlabelled data into a certain number (thats the \" K\") of distinct groupings. K-means finds observations that share important characteristics and classifies them together into clusters.\n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612125/EDA/n6d4tc4qgsucpxcgbl3p.gif' width=\"600\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecd24f-e42c-4875-8759-d4ef56da9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d48dcd-0469-46e4-9c78-ce402f75480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "km_data = km.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4471c-8a93-47b2-bab5-f013e587ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "km_labels = np.array([iris.target_names[val] for val in km.labels_])\n",
    "fig, ax = plt.subplots(1,1 ,figsize=[8,6])\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=km_labels, palette='tab10', ax=ax, legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,0], y=km.cluster_centers_[:,1], color='red',marker='*', s=300, label='centroid')\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[0])\n",
    "ax.set_ylabel(iris.feature_names[1])\n",
    "ax.set_title('No clustering')\n",
    "ax.set_title('KMeans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930c2dc-a9eb-4848-9ab7-d88c2f561190",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_labels = np.array([iris.target_names[val] for val in km.labels_])\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=[10,5])\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=km_labels, palette='tab10', ax=ax[0][0], legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,0], y=km.cluster_centers_[:,1], color='red',marker='*', s=300,ax=ax[0][0])\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,2], hue=km_labels, palette='tab10', ax=ax[0][1], legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,0], y=km.cluster_centers_[:,2], color='red',marker='*', s=300,ax=ax[0][1])\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,3], hue=km_labels, palette='tab10', ax=ax[0][2], legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,0], y=km.cluster_centers_[:,3], color='red',marker='*', s=300,ax=ax[0][2])\n",
    "\n",
    "sns.scatterplot(x=X[:,1], y=X[:,2], hue=km_labels, palette='tab10', ax=ax[1][0], legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,1], y=km.cluster_centers_[:,2], color='red',marker='*', s=300,ax=ax[1][0])\n",
    "\n",
    "sns.scatterplot(x=X[:,1], y=X[:,3], hue=km_labels, palette='tab10', ax=ax[1][1], legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,1], y=km.cluster_centers_[:,3], color='red',marker='*', s=300,ax=ax[1][1])\n",
    "\n",
    "sns.scatterplot(x=X[:,2], y=X[:,3], hue=km_labels, palette='tab10', ax=ax[1][2], legend=False)\n",
    "sns.scatterplot(x=km.cluster_centers_[:,2], y=km.cluster_centers_[:,3], color='red',marker='*', s=300,ax=ax[1][2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e195cfc",
   "metadata": {},
   "source": [
    "### How many clusters would we need?\n",
    "\n",
    "There is no perfect solution to choosing k but one popular heuristic is known as the _elbow approach_. This involves applying k-means for a range of values of k and plotting the choice of k against the _total sum of squared distances (SSD).\n",
    "\n",
    "* **SSD will always decrease with the increase of the number of clusters**\n",
    "* **we are looking to find the point when adding more clusters no longer provides a significant decrease in in SSD (longer improves our overall solution)** \n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612126/EDA/vs6hgvuqydpqbj2txcnq.gif' width=\"700\" style=\"float:center\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261338ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in sci-kit learn the sum of squared distances is called Inertia\n",
    "ssd = [] \n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    ssd.append(kmeans.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "plt.scatter(range(1,11), ssd)\n",
    "plt.plot(range(1,11), ssd)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb5988",
   "metadata": {},
   "source": [
    "## Combining PCA and K-means Clustering\n",
    "\n",
    "* Reducing the number of features improves the performance of the algorithm. \n",
    "* By decreasing the number of features the noise is also reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e934fe-fb51-4eae-a3b9-644565219807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca_data = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75449281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Means\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
    "pca_kmeans = kmeans.fit(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cbf5b-b654-4b93-9725-3b75ddd659b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting PCA and PCA + KMeans\n",
    "pca_kmeans_labels = np.array([iris.target_names[val] for val in kmeans.labels_])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[12,5])\n",
    "\n",
    "#PCA\n",
    "sns.scatterplot(x=pca_data[:,0], y=pca_data[:,1], hue=species, palette='tab10', ax=ax[0])\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[0].set_title('PCA')\n",
    "\n",
    "#PCA + KMEANS\n",
    "sns.scatterplot(x=pca_data[:,0], y=pca_data[:,1], hue=pca_kmeans_labels, palette='tab10', ax=ax[1])\n",
    "sns.scatterplot(x=kmeans.cluster_centers_[:,0], y=kmeans.cluster_centers_[:,1], color='red',marker='*', s=300, label='centroid',ax=ax[1])\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[1].set_ylabel('PC2')\n",
    "ax[1].set_title('PCA + KMeans')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157c414-78c3-4ac0-80b5-72500c8282bd",
   "metadata": {},
   "source": [
    "### Silhouette Score\n",
    "\n",
    "is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1:\n",
    "\n",
    "* **1:** Means clusters are well apart from each other and clearly distinguished.\n",
    "* **0:** Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
    "* **-1:** Means clusters are assigned in the wrong way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e09fb-592c-4470-9cd4-548b0533e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "for k in range(2,8):\n",
    "    iriskmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "    labels = iriskmeans.labels_\n",
    "    sc = silhouette_score(X, labels, metric='euclidean')\n",
    "    print(k, round(sc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ece2b9-5020-472a-846b-a975da49082c",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "**DBSCAN** stands for _Density-based Spatial Clustering of Applications with Noise_. \n",
    "\n",
    "* Unsupervised clustering algorithm\n",
    "* Uses density to gather points in space to form clusters \n",
    "* Detects automatically the number of clusters based on your input data and parameters\n",
    "* Can handle noise and outliers\n",
    "  \n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/inkh3h3snoa5q6mh0ic2.png' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "### The DBSCAN algorithm uses two major parameters:\n",
    "\n",
    "* **minPts:** The minimum number of points (a threshold) clustered together for a region to be considered dense i.e. the minimum number of data points that can form a cluster\n",
    "* **eps (Îµ):** Maximum radius of the neighborhood.\n",
    "\n",
    "The algorithm takes care of two concepts:\n",
    "\n",
    "* **Density Reachability:** a point to be reachable from another if it lies within a particular distance (eps) from it, which indicates how densely reachable a cluster is.\n",
    "* **Density Connectivity:** refers to the idea that points are connected if they are density-reachable from each other.\n",
    "\n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/nddkjdbqmawal7zjt7ox.png' width=\"500\" style=\"float:center\" />\n",
    "<br>\n",
    "\n",
    "- **Core point:** any point with a neighbor count greater than or equal to MinPts\n",
    "- **Border point:** a point with a neighbor count lower than MinPts, but thant belongs to the Ïµ\n",
    "- **Outlier:** is neither a core nor a border point\n",
    "\n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/cfmvzbj9gna6fynp6w0g.gif' width=\"300\" style=\"float:center\" />\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c3fe0-209f-48ee-8f30-58a1e091edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027a297-e514-4e54-b969-a21289a9441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan = db.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b78e3-d2f4-4645-98b4-53f9777064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbscan_labels = np.array([iris.target_names[val] for val in dbscan.labels_])\n",
    "fig, ax = plt.subplots(1,1 ,figsize=[6,5])\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=dbscan.labels_, palette='tab10', ax=ax, legend=True)\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[0])\n",
    "ax.set_ylabel(iris.feature_names[1])\n",
    "ax.set_title('DBSCAN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828f9f1-84ba-4db6-af02-f383e1480578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=[10,5])\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=dbscan.labels_, palette='tab10', ax=ax[0][0], legend=False)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,2], hue=dbscan.labels_, palette='tab10', ax=ax[0][1], legend=False)\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,3], hue=dbscan.labels_, palette='tab10', ax=ax[0][2], legend=False)\n",
    "\n",
    "sns.scatterplot(x=X[:,1], y=X[:,2], hue=dbscan.labels_, palette='tab10', ax=ax[1][0], legend=False)\n",
    "\n",
    "sns.scatterplot(x=X[:,1], y=X[:,3], hue=dbscan.labels_, palette='tab10', ax=ax[1][1], legend=False)\n",
    "\n",
    "sns.scatterplot(x=X[:,2], y=X[:,3], hue=dbscan.labels_, palette='tab10', ax=ax[1][2], legend=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bfe5ef-281c-4248-bb1b-f4dc18f33d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_pca = DBSCAN(eps=0.5, min_samples=13)\n",
    "dbscan_pca = db_pca.fit(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0fdb1-4ef4-435f-8e49-9d171d53d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting PCA and PCA + KMeans\n",
    "#pca_kmeans_labels = np.array([iris.target_names[val] for val in kmeans.labels_])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[12,5])\n",
    "\n",
    "#PCA\n",
    "sns.scatterplot(x=pca_data[:,0], y=pca_data[:,1], hue=species, palette='tab10', ax=ax[0])\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[0].set_title('PCA')\n",
    "\n",
    "#PCA + KMEANS\n",
    "sns.scatterplot(x=pca_data[:,0], y=pca_data[:,1], hue=dbscan_pca.labels_, palette='tab10', ax=ax[1])\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[1].set_ylabel('PC2')\n",
    "ax[1].set_title('PCA + DBSCAN')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27800e-604c-4b46-9dd9-2789cc84b94b",
   "metadata": {},
   "source": [
    "K-means clustering does better with the iris data, because it allows us to specify the number of clusters. Where DBSCAN really excels is with irregularly-shaped clusters (even very irregularly-shaped) that are well-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6b88e-0fb7-47da-bbdb-c64ac1ec0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for minpt in range(2,12):\n",
    "    irisdbscan = DBSCAN(min_samples = minpt).fit(X)\n",
    "    core_indices = irisdbscan.core_sample_indices_\n",
    "    labels = irisdbscan.labels_[core_indices]\n",
    "    nclusters = len(set(labels))\n",
    "    sc = silhouette_score(X[core_indices], labels, metric='euclidean')\n",
    "    print(minpt, nclusters, round(sc,4), sum([1 for x in irisdbscan.labels_ if x == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb86d9-315e-4a40-805b-cf7fcd58dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(min_samples=13)\n",
    "dbscan = db.fit(iris.data)\n",
    "\n",
    "#dbscan_labels = np.array([iris.target_names[val] for val in dbscan.labels_])\n",
    "fig, ax = plt.subplots(1,1 ,figsize=[6,5])\n",
    "sns.scatterplot(x=iris.data[:,0], y=iris.data[:,1], hue=dbscan.labels_, palette='tab10', ax=ax, legend=True)\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[0])\n",
    "ax.set_ylabel(iris.feature_names[1])\n",
    "ax.set_title('DBSCAN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c80768-5ea1-4403-9ddb-02324b0a8268",
   "metadata": {},
   "source": [
    "# Distances Measures\n",
    "\n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/a1ensjlzi9zjzh58oyi8.png' width=\"600\" style=\"float:center\" />\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "compbio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
