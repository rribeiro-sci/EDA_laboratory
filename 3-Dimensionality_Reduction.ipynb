{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcbec11",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h1 style = \"font-size:50px; font-family:Helvetica ; font-weight : normal; color : #fe346e; text-align: center;\"> Exploratory Data Analysis</h1>\n",
    "<h2 style = \"font-size:40px; font-family:Helvetica ; font-weight : normal; text-align: center;\"> Dimensionality Reduction </h2>\n",
    "<br><br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612126/EDA/dw1xwbh1g2c85izletjs.png' width=\"200\"  style=\"float:center\" align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style='padding:15px'>\n",
    "<a href=\"https://colab.research.google.com/github/rribeiro-sci/EDA_laboratory/blob/main/3-Dimensional_Reduction.ipynb\" target=\"_blank\">\n",
    "<img alt=\"Colab\" src=\"https://res.cloudinary.com/djz27k5hg/image/upload/v1637335713/badges/colab-badge_hh0uyl.svg\" height=\"25\" style=\"margin:20px\">\n",
    "</a>\n",
    "\n",
    "</div\n",
    "\n",
    "Dimensionality Reduction techniques play a crucial role in simplifying complex datasets while preserving essential information. In this tutorial, we will explore the following techniques, understand their principles, and learn how to implement them using Python:\n",
    "\n",
    "* **Principal Component Analysis (PCA)**\n",
    "* **Linear Discriminant Analysis (LDA)**\n",
    "* **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "\n",
    "In this tutorial we will use the Iris Dataset. This is the gold-standard benchmark dataset in the field of machine learning, and is often used for demonstrating various algorithms and techniques. It consists of measurements of four features:\n",
    "\n",
    "* sepal length \n",
    "* sepal width \n",
    "* petal length\n",
    "* petal width\n",
    "\n",
    "of three species of iris flowers: Setosa, Versicolor, and Virginica. \n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/hgt6fcp5o6lrfnliqlfr.jpg' width=\"300\" style=\"float:center\" />\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612125/EDA/t7sfxggoz9mzozahhrsn.png' width=\"900\" style=\"float:center\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn import datasets\n",
    "\n",
    "# importing iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35cbb7",
   "metadata": {},
   "source": [
    "**How does the dataset looks like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris.data\n",
    "#iris.feature_names\n",
    "#iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a19df",
   "metadata": {},
   "source": [
    "we can create a dataframe from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb74583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "\n",
    "# loading iris dataset into a dataframe\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb545e7-5993-4897-9a22-afe5781ff213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# creating the column \"species\"\n",
    "iris_df['species']=np.array([iris.target_names[val] for val in iris.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77515b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# setting jupyter plotting parameters\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Ploting two features\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[6,5])\n",
    "#plt.figure(figsize=(6,5))\n",
    "\n",
    "sns.scatterplot(data =iris_df,\n",
    "                x='sepal length (cm)', \n",
    "                y='sepal width (cm)',\n",
    "                hue='species', \n",
    "                palette='tab10', legend='full');\n",
    "\n",
    "# Legend\n",
    "plt.legend(title='')\n",
    "\n",
    "#styling\n",
    "plt.xlabel('sepal length (cm)', fontsize=16, labelpad=20)\n",
    "plt.ylabel('sepal width (cm)', fontsize=16, labelpad=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "sns.pairplot(iris_df, hue='species',diag_kind=\"hist\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1dfa1-516d-49ee-8f40-1ab1557c01a9",
   "metadata": {},
   "source": [
    "### **Is there a way of taking into account all variables?**\n",
    "\n",
    "An amazing solution if the _**P**rinciple **C**omponent **A**nalysis_ (**PCA**). PCA takes all variables of the dataset and combines them producing new factors called _**P**inciple **C**omponents_ (**PC**). And it does in such a way that PCs keep most of the information of the dataset.\n",
    "\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772301/EDA/yz2mcnkpnwvicb8xo6jb.png' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772301/EDA/hkdu9mhquhok2brn1scb.png' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772301/EDA/lnpbsqvlsq4rkft7n63i.png' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "To not overvalue those dimensions with the highest numerical values we need to normalize and center the values of every input dimenison before applying the PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91f285-216b-45ec-a5ce-c297e7c231c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # for data standardization\n",
    "\n",
    "# Get scaler\n",
    "scaler=StandardScaler()\n",
    "# Perform standard scaling on model features\n",
    "X=scaler.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df6997-919a-4d73-b4a3-d85fa3a14848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Creating a PCA instance\n",
    "pca = PCA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7435a6-2e3c-46c0-a19e-75792cb6c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the PCA model with our data\n",
    "pca.fit(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e4c28-c9e3-4d3f-8e83-ce93b6ec45b8",
   "metadata": {},
   "source": [
    "When you call the fit() method on the PCA object, you're essentially training the PCA model on your dataset. During this process, PCA analyzes the structure of your data and computes the principal components. These principal components are the new orthogonal axes in the feature space that capture the directions of maximum variance in the data.\n",
    "\n",
    "Here's what happens in detail:\n",
    "\n",
    "- The PCA algorithm calculates the mean of each feature in your dataset.\n",
    "- It then centers the data by subtracting the mean from each feature. This step ensures that the principal components are not biased towards any particular feature.\n",
    "- Next, PCA computes the covariance matrix of the centered data. The covariance matrix describes the relationships between the different features in your dataset.\n",
    "- Finally, PCA performs eigendecomposition on the covariance matrix to extract the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41252c82-d162-4c54-8094-baedc7512cd9",
   "metadata": {},
   "source": [
    "The next step is projecting the original data points onto the new principal component axes. To do that with the `transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb56edd-4965-47a1-a412-152032eef0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying transformation\n",
    "iris_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b777cc7-0513-40b1-b427-b58f1becb8a7",
   "metadata": {},
   "source": [
    "We can now plot the new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae071a3-e55b-44a9-9789-4a6f6d4c2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ploting two features\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[6,5])\n",
    "\n",
    "sns.scatterplot(x=iris_pca[:,0], y=iris_pca[:,1], hue=iris_df.species)\n",
    "\n",
    "# Legend\n",
    "plt.legend(title='')\n",
    "\n",
    "#styling\n",
    "plt.xlabel('PC1', fontsize=16, labelpad=20)\n",
    "plt.ylabel('PC2', fontsize=16, labelpad=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4f74f-334c-4c89-a505-82729b27b62d",
   "metadata": {},
   "source": [
    "### How about the other PCs?\n",
    "* **How many PC should be calculated?**\n",
    "* **How do we know if the first two principal components are enough to capture most of the information or variance in the dataset?**\n",
    "\n",
    "The **Scree Plot** tells us how much variance of the dataset is explained by each PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71fffe-fd6c-4219-89d0-a71f69936eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597890b3-9020-4b7f-9d92-960f86d2364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing scree plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, color='b', alpha=0.5, align='center')\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), color='r', marker='o', linestyle='-')\n",
    "\n",
    "#Styling\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b97264-c7ec-447d-80cf-9a92e93e200d",
   "metadata": {},
   "source": [
    "Ideally, we want to get around 90% variance with just 2 to 3 PCs to retain enough information while we can still visualize our data on a plot.\n",
    "\n",
    "### **But was it exactly PC1 and PC2? What does that mean?**\n",
    "\n",
    "We are interested in which factors of our dataset contribute to the PCs. And how variables are correlated. This is given by the principal components **LOADINGS**. Basically, each variable gets a loading (or weight) for each PC, which tells us how much it contributes to that PC. We can also plot the loadings to see the relationship between the variables. \n",
    "\n",
    "Essentially, PC **LOADINGS**, let us know:\n",
    "- Which variables are influential\n",
    "- How variables are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ea995-ce2f-43ce-94b2-5bc406f8d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geting loadings\n",
    "loadings = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e6689-0b1d-47cb-9ccb-879cb2d932c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "sns.scatterplot(x=loadings[0], y=loadings[1] )\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([-1,1])\n",
    "\n",
    "# Styling \n",
    "plt.xlabel('PC1', fontsize=16, labelpad=20)\n",
    "plt.ylabel('PC2', fontsize=16, labelpad=20)\n",
    "# Annotate each point with its name\n",
    "for i, name in enumerate(iris.feature_names):\n",
    "    plt.annotate(name, (loadings[0][i], loadings[1][i]))\n",
    "\n",
    "plt.axvline(0, c='black', ls='--') # adding vertical line in data co-ordinates\n",
    "plt.axhline(0, c='black', ls='--') # adding horizontal line in data co-ordinates\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324e26f-f046-4ef3-b7ba-c52ddfe5c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf8e3e-dd5a-40d8-ba93-1841d0281735",
   "metadata": {},
   "source": [
    "**Magnitude of Loading Values:** The loading values indicate the strength of the relationship between each variable and each principal component. Higher absolute values suggest a stronger relationship.\n",
    "\n",
    "**Direction of Loading Values:** The direction of the loading values indicates the correlation between variables and principal components. Variables pointing in the same direction have a positive correlation with each other, while those pointing in opposite directions have a negative correlation.\n",
    "\n",
    "**Proximity of Variables:** Variables that are close together in the plot have similar patterns of variation across observations. Variables far apart have dissimilar patterns.\n",
    "\n",
    "\n",
    "#### How to interpret the PCA plot?\n",
    "\n",
    "In a PCA plot:\n",
    "* Each dot represents an entry in the dataset.\n",
    "* Entries with similar feature profiles cluster together.\n",
    "* PC1 > PC2 in describing data.\n",
    "\n",
    "\n",
    "\n",
    "Looking at the PCA plot (example) below:\n",
    "* Three distinct clusters of entries are evident.\n",
    "* Orange and green clusters differ along PC1.\n",
    "* Pink and green clusters differ along PC2.\n",
    "* Greater difference along PC1 axis between green and orange clusters than along PC2 axis between pink and green clusters.\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/hnsml2bkszm9jkuafytd.png' width=\"800\" style=\"float:center\" />\n",
    "\n",
    "\n",
    "\n",
    "### Linear Desctiminant Analysis\n",
    "\n",
    "PCA reduces dimensions by focusing on features with the most variation. However, in this case we are more interested in maximizing the separability between the 3 species. **Linear Discriminant Analysis** (LDA) is similar to PCA: it reduce dimentions but focus on **maximizing the separability among categories**.\n",
    "\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772302/EDA/qf9lffgyyz09iqertj5z.png' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772301/EDA/o1dcnegrvd6x6ywv7smg.png' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772301/EDA/q6tcwwxcl8bx9cscfrgq.png' width=\"700\" style=\"float:center\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19f233-8d8e-490e-8ad9-13f9aa8662de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989c4f5-06a1-4a2d-b3e0-50ef7d03b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(X, iris.target) # in this case we also need to include categories because LDA is a supervised dimensionality reduction.\n",
    "\n",
    "irislda = lda.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf14403-f4cb-4ec0-80c1-c1397f1a5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(irislda[:,0], irislda[:,1], c=iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c0086-781f-4e71-97cf-5a461a15d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 4])\n",
    "\n",
    "\n",
    "#ax1.scatter(irislda[:,0], irislda[:,1], c=iris.target)\n",
    "\n",
    "# Plot data1 on the first subplot\n",
    "sns.scatterplot(x=iris_pca[:,0], y=iris_pca[:,1], hue=iris_df.species, ax=ax1)\n",
    "ax1.set_title('PCA')\n",
    "\n",
    "# Plot data2 on the second subplot\n",
    "sns.scatterplot(x=irislda[:, 0], y=irislda[:, 1], hue= iris_df.species, ax=ax2)\n",
    "ax2.set_title('LDA')\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7136f0-e31f-446c-8289-1fb63120620e",
   "metadata": {},
   "source": [
    "### What did ou learn from PCA and LDA applied to the iris dataset?\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680858c9-f06f-48af-aac0-85c332aec988",
   "metadata": {},
   "source": [
    "## What if you couldn't separate our data by any straight line?\n",
    "\n",
    "t-SNE is an algorithm that deals with linearly nonseparable data. t-SNE stands for t-distributed Stochastic Neighbor Embedding, which tells the following :\n",
    "* Stochastic → not definite but random probability\n",
    "* Neighbor →concerned only about retaining the variance of neighbor points\n",
    "* Embedding → plotting data into lower dimensions\n",
    "\n",
    "t-SNE is a machine learning algorithm that generates slightly different results each time on the same data set, focusing on retaining the structure of neighbor points.\n",
    "\n",
    "**How does t-SNE work?**\n",
    "\n",
    "Step 1: t-SNE constructs a probability distribution on pairs in higher dimensions such that similar objects are assigned a higher probability and dissimilar objects are assigned lower probability.\n",
    "\n",
    "Step 2: Then, t-SNE replicates the same probability distribution on lower dimensions iteratively till the Kullback-Leibler divergence is minimized.\n",
    "\n",
    "Kullback-Leibler divergence is a measure of the difference between the probability distributions from Step1 and Step2. KL divergence is mathematically given as the expected value of the logarithm of the difference of these probability distributions.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715772951/EDA/qfcowlphpafelwgvtnhu.webp' width=\"700\" style=\"float:center\" />\n",
    "\n",
    "<br>\n",
    "<img src='https://res.cloudinary.com/djz27k5hg/image/upload/v1715612124/EDA/bvyten7bwul5b4xygoii.gif' width=\"700\" style=\"float:center\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39e786-ca99-4cb4-a6fb-51d47800f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, init='random', perplexity=30,learning_rate='auto',)\n",
    "tsne.fit(X)\n",
    "iristsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(iristsne[:,0], iristsne[:,1], c=iris.target);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8eba6-3009-4b99-b320-0796147a0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(X, sigma):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian kernel matrix for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data matrix with shape (n_samples, n_features).\n",
    "    - sigma: Bandwidth parameter for the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "    - similarities: Similarity matrix computed using the Gaussian kernel.\n",
    "    \"\"\"\n",
    "    distances_sq = np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1)\n",
    "    similarities = np.exp(-distances_sq / (2 * sigma ** 2))\n",
    "    return similarities\n",
    "\n",
    "def student_kernel(X, sigma):\n",
    "    \"\"\"\n",
    "    Compute the Student's t-distribution kernel matrix for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data matrix with shape (n_samples, n_features).\n",
    "    - sigma: Bandwidth parameter for the kernel.\n",
    "\n",
    "    Returns:\n",
    "    - similarities: Similarity matrix computed using the Student's t-distribution kernel.\n",
    "    \"\"\"\n",
    "    distances_sq = np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1)\n",
    "    similarities = 1 / (1 + distances_sq / sigma ** 2)\n",
    "    np.fill_diagonal(similarities, 0)  # Set diagonal elements to 0\n",
    "    return similarities\n",
    "# Sample data\n",
    "\n",
    "# Fit t-SNE to the data\n",
    "tsne = TSNE(n_components=2, init='pca')\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "# Compute similarity matrices\n",
    "sigma = 1.0  # Bandwidth parameter for the kernel\n",
    "similarities_original = gaussian_kernel(X, sigma)\n",
    "similarities_tsne = student_kernel(X_embedded, sigma)\n",
    "\n",
    "# Plot the matrices side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original data similarity matrix\n",
    "axes[0].imshow(similarities_original, cmap='viridis')\n",
    "axes[0].set_title('Original Data Similarity Matrix')\n",
    "axes[0].set_xlabel('Data Points')\n",
    "axes[0].set_ylabel('Data Points')\n",
    "\n",
    "# t-SNE embeddings similarity matrix\n",
    "im1 = axes[1].imshow(similarities_tsne, cmap='viridis')\n",
    "axes[1].set_title('t-SNE Embeddings Similarity Matrix')\n",
    "axes[1].set_xlabel('Data Points')\n",
    "axes[1].set_ylabel('Data Points')\n",
    "\n",
    "plt.colorbar(im1, ax=axes[1], label='Similarity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08480a9d-e2bf-4ea4-95cf-e5e44d1ef00e",
   "metadata": {},
   "source": [
    "## t-SME hyperparameters really matter\n",
    "\n",
    "### How _perplexity_ influence sthe resutls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4df36-e741-4e0f-934a-bb4b90835d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne(data,perplexity, step):\n",
    "    tsne = TSNE(n_components=2, init='random', perplexity=perplexity,learning_rate='auto', n_iter=step)\n",
    "    tsne_results = tsne.fit_transform(data)\n",
    "    return tsne_results\n",
    "    \n",
    "tsne1 = run_tsne(X,2, 5000)\n",
    "tsne2 = run_tsne(X,5, 5000)\n",
    "tsne3 = run_tsne(X,30, 5000)\n",
    "tsne4 = run_tsne(X,50, 5000)\n",
    "tsne5 = run_tsne(X,100, 5000)\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=[25,4])\n",
    "\n",
    "sns.scatterplot(x=tsne1[:,0], y=tsne1[:,1], hue=iris_df.species, ax=ax[0], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne2[:,0], y=tsne2[:,1], hue=iris_df.species, ax=ax[1], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne3[:,0], y=tsne3[:,1], hue=iris_df.species, ax=ax[2], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne4[:,0], y=tsne4[:,1], hue=iris_df.species, ax=ax[3], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne5[:,0], y=tsne5[:,1], hue=iris_df.species, ax=ax[4], palette='tab10',legend=False);\n",
    "\n",
    "ax[0].set_title('Perplexity: 2 \\n step: 5000')\n",
    "ax[1].set_title('Perplexity: 5 \\n step: 5000')\n",
    "ax[2].set_title('Perplexity: 30 \\n step: 5000')\n",
    "ax[3].set_title('Perplexity: 50 \\n step: 5000')\n",
    "ax[4].set_title('Perplexity: 100 \\n step: 5000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60637c-adeb-4204-b425-3eca722c3b74",
   "metadata": {},
   "source": [
    "### How the the number of steps influence the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67625616-a4dc-4c09-b5b7-4fb9321435e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne1 = run_tsne(X,30, 250)\n",
    "tsne2 = run_tsne(X,30, 500)\n",
    "tsne3 = run_tsne(X,30, 1000)\n",
    "tsne4 = run_tsne(X,30, 5000)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=[20,4])\n",
    "\n",
    "sns.scatterplot(x=tsne1[:,0], y=tsne1[:,1], hue=iris_df.species, ax=ax[0], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne2[:,0], y=tsne2[:,1], hue=iris_df.species, ax=ax[1], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne3[:,0], y=tsne3[:,1], hue=iris_df.species, ax=ax[2], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne4[:,0], y=tsne4[:,1], hue=iris_df.species, ax=ax[3], palette='tab10',legend=False);\n",
    "\n",
    "ax[0].set_title('Perplexity: 30 \\n step: 250')\n",
    "ax[1].set_title('Perplexity: 30 \\n step: 500')\n",
    "ax[2].set_title('Perplexity: 30 \\n step: 1000')\n",
    "ax[3].set_title('Perplexity: 30 \\n step: 5000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a2f8f-c96a-4bb8-8d48-2f9ce4743119",
   "metadata": {},
   "source": [
    "### t-SNE is not deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c4f45-99f9-4ec9-93c1-d7793bfece98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne1 = run_tsne(X,30, 5000)\n",
    "tsne2 = run_tsne(X,30, 5000)\n",
    "tsne3 = run_tsne(X,30, 5000)\n",
    "tsne4 = run_tsne(X,30, 5000)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=[20,4])\n",
    "\n",
    "sns.scatterplot(x=tsne1[:,0], y=tsne1[:,1], hue=iris_df.species, ax=ax[0], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne2[:,0], y=tsne2[:,1], hue=iris_df.species, ax=ax[1], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne3[:,0], y=tsne3[:,1], hue=iris_df.species, ax=ax[2], palette='tab10',legend=False);\n",
    "sns.scatterplot(x=tsne4[:,0], y=tsne4[:,1], hue=iris_df.species, ax=ax[3], palette='tab10',legend=False);\n",
    "\n",
    "ax[0].set_title('Perplexity: 30 \\n step: 5000')\n",
    "ax[1].set_title('Perplexity: 30 \\n step: 5000')\n",
    "ax[2].set_title('Perplexity: 30 \\n step: 5000')\n",
    "ax[3].set_title('Perplexity: 30 \\n step: 5000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af255ff3-ff6f-43f1-8f06-8d5c2511194c",
   "metadata": {},
   "source": [
    "# PCA vs. LDA vs. t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfb236-ac79-4c3d-b2e0-9f4bb49c405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=[20,5])\n",
    "\n",
    "sns.scatterplot(x=iris_pca[:,0],y=iris_pca[:,1], hue=iris_df.species, palette='tab10', ax=ax[0])\n",
    "sns.scatterplot(x=irislda[:,0],y=irislda[:,1], hue=iris_df.species, palette='tab10', ax=ax[1], legend=False)\n",
    "sns.scatterplot(x=iristsne[:,0],y=iristsne[:,1], hue=iris_df.species, palette='tab10', ax=ax[2], legend=False)\n",
    "\n",
    "\n",
    "ax[0].set_title('PCA')\n",
    "ax[1].set_title('LDA')\n",
    "ax[2].set_title('t-SNE')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "compbio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
